#!/usr/bin/env python3
"""
æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Brave, DuckDuckGo, Bingã®å„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§ç°¡å˜ãªãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
"""

import requests
from bs4 import BeautifulSoup
import time
import sys
from urllib.parse import urlencode


def test_brave_search(query: str):
    """Braveæ¤œç´¢ã®ãƒ†ã‚¹ãƒˆ"""
    print(f"\n=== Braveæ¤œç´¢ãƒ†ã‚¹ãƒˆ: '{query}' ===")
    
    try:
        # Braveæ¤œç´¢URL
        url = f"https://search.brave.com/search?q={query}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: {len(response.text)} æ–‡å­—")
        
        if response.status_code != 200:
            print(f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
        selectors = [
            '.snippet',
            '[data-type="web"]',
            '.result',
            '.web-result',
            'article',
            '.fdb'
        ]
        
        results = []
        for selector in selectors:
            elements = soup.select(selector)
            print(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}': {len(elements)}ä»¶")
            
            if elements:
                for i, elem in enumerate(elements[:3]):
                    title_elem = elem.find('a')
                    title = title_elem.get_text(strip=True) if title_elem else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
                    url = title_elem.get('href', '') if title_elem else ""
                    
                    if title and len(title) > 10:
                        results.append({
                            'title': title,
                            'url': url,
                            'selector': selector
                        })
                        print(f"  {i+1}. {title[:50]}...")
                        if url:
                            print(f"      URL: {url}")
                
                if results:
                    break
        
        if not results:
            print("âŒ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
            title = soup.title.string if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        else:
            print(f"âœ… {len(results)}ä»¶ã®çµæœã‚’å–å¾—")
        
        return results
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []


def test_duckduckgo_search(query: str):
    """DuckDuckGoæ¤œç´¢ã®ãƒ†ã‚¹ãƒˆ"""
    print(f"\n=== DuckDuckGoæ¤œç´¢ãƒ†ã‚¹ãƒˆ: '{query}' ===")
    
    try:
        # DuckDuckGoæ¤œç´¢URL
        url = f"https://html.duckduckgo.com/html/?q={query}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: {len(response.text)} æ–‡å­—")
        
        if response.status_code != 200:
            print(f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # DuckDuckGoç‰¹æœ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
        selectors = [
            '.result',
            '.web-result',
            '.links_main',
            '.result__body',
            '.result__snippet',
            'h2.result__title',
            '.results .result'
        ]
        
        results = []
        for selector in selectors:
            elements = soup.select(selector)
            print(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}': {len(elements)}ä»¶")
            
            if elements:
                for i, elem in enumerate(elements[:3]):
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
                    title_elem = elem.find('a', class_='result__a') or elem.find('h2') or elem.find('a')
                    title = title_elem.get_text(strip=True) if title_elem else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
                    url = title_elem.get('href', '') if title_elem else ""
                    
                    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’æ¢ã™
                    snippet_elem = elem.find(class_='result__snippet') or elem.find('p')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    if title and len(title) > 10:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'selector': selector
                        })
                        print(f"  {i+1}. {title[:50]}...")
                        if snippet:
                            print(f"      {snippet[:60]}...")
                        if url:
                            print(f"      URL: {url}")
                
                if results:
                    break
        
        if not results:
            print("âŒ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
            title = soup.title.string if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        else:
            print(f"âœ… {len(results)}ä»¶ã®çµæœã‚’å–å¾—")
        
        return results
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []


def test_bing_search(query: str):
    """Bingæ¤œç´¢ã®ãƒ†ã‚¹ãƒˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    print(f"\n=== Bingæ¤œç´¢ãƒ†ã‚¹ãƒˆ: '{query}' ===")
    
    try:
        # Bingæ¤œç´¢URL
        params = {
            'q': query,
            'count': '10',
            'mkt': 'ja-JP',
            'setlang': 'ja'
        }
        url = f"https://www.bing.com/search?{urlencode(params)}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {response.status_code}")
        print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚µã‚¤ã‚º: {len(response.text)} æ–‡å­—")
        
        if response.status_code != 200:
            print(f"âŒ HTTPã‚¨ãƒ©ãƒ¼: {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Bingç”¨ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
        selectors = [
            '.b_algo',
            'li.b_algo',
            '.b_searchResult',
            '.b_title',
            'h2 a',
            'a[href^="http"]:not([href*="bing.com"])',
            '[data-priority]'
        ]
        
        results = []
        for selector in selectors:
            elements = soup.select(selector)
            print(f"ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}': {len(elements)}ä»¶")
            
            if elements and selector in ['.b_algo', 'li.b_algo', '.b_searchResult']:
                # æ§‹é€ åŒ–ã•ã‚ŒãŸæ¤œç´¢çµæœ
                for i, elem in enumerate(elements[:3]):
                    title_elem = elem.select_one('h2 a') or elem.select_one('a')
                    title = title_elem.get_text(strip=True) if title_elem else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
                    url = title_elem.get('href', '') if title_elem else ""
                    
                    snippet_elem = elem.select_one('.b_caption p') or elem.select_one('p')
                    snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                    
                    if title and len(title) > 10:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'selector': selector
                        })
                        print(f"  {i+1}. {title[:50]}...")
                        if snippet:
                            print(f"      {snippet[:60]}...")
                        if url:
                            print(f"      URL: {url}")
                
                if results:
                    break
            
            elif selector == 'a[href^="http"]:not([href*="bing.com"])' and not results:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤–éƒ¨ãƒªãƒ³ã‚¯ã‚’ç›´æ¥æ¢ã™
                seen_urls = set()
                for i, elem in enumerate(elements[:5]):
                    href = elem.get('href', '')
                    title = elem.get_text(strip=True)
                    
                    if href and href not in seen_urls and title and len(title) > 10:
                        seen_urls.add(href)
                        results.append({
                            'title': title,
                            'url': href,
                            'snippet': "å¤–éƒ¨ãƒªãƒ³ã‚¯ã‹ã‚‰å–å¾—",
                            'selector': selector
                        })
                        print(f"  {i+1}. {title[:50]}...")
                        print(f"      URL: {href}")
                        
                        if len(results) >= 3:
                            break
        
        if not results:
            print("âŒ æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
            title = soup.title.string if soup.title else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
            print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title}")
            
            # åˆ©ç”¨å¯èƒ½ãªè¦ç´ ã‚’ã„ãã¤ã‹è¡¨ç¤º
            all_links = soup.find_all('a', href=True)
            external_links = [a for a in all_links if a.get('href', '').startswith('http') and 'bing.com' not in a.get('href', '')]
            print(f"å¤–éƒ¨ãƒªãƒ³ã‚¯ç·æ•°: {len(external_links)}")
        else:
            print(f"âœ… {len(results)}ä»¶ã®çµæœã‚’å–å¾—")
        
        return results
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "å²¸ç”°æ–‡é›„ èª•ç”Ÿæ—¥",
        "Python ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°",
        "å¤©æ°— æ±äº¬"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª: '{query}'")
        print("=" * 50)
        
        # å„æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ãƒ†ã‚¹ãƒˆ
        brave_results = test_brave_search(query)
        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        duckduckgo_results = test_duckduckgo_search(query)
        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        bing_results = test_bing_search(query)
        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        
        # çµæœã¾ã¨ã‚
        print(f"\nğŸ“Š '{query}' ã®çµæœã¾ã¨ã‚:")
        print(f"  Brave: {len(brave_results)}ä»¶")
        print(f"  DuckDuckGo: {len(duckduckgo_results)}ä»¶")
        print(f"  Bing: {len(bing_results)}ä»¶")
        
        # ã©ã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãŒæœ€ã‚‚è‰¯ã„çµæœã‚’è¿”ã—ãŸã‹
        best_engine = "ãªã—"
        max_results = 0
        
        for engine, results in [("Brave", brave_results), ("DuckDuckGo", duckduckgo_results), ("Bing", bing_results)]:
            if len(results) > max_results:
                max_results = len(results)
                best_engine = engine
        
        if max_results > 0:
            print(f"  æœ€è‰¯: {best_engine} ({max_results}ä»¶)")
        else:
            print("  âš ï¸ ã™ã¹ã¦ã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§çµæœãªã—")
        
        print("-" * 50)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)